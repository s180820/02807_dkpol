{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b9d670f9-28bf-4981-8f9e-cf22056d423f",
    "deepnote_cell_height": 436,
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3792,
    "execution_start": 1668958920397,
    "source_hash": "522fcc3f"
   },
   "source": [
    "# TODO\n",
    "\n",
    "- lav similarity matrix på lsh pakken som er i deepnote \n",
    "- repliker lsh pakkens resultater i løsningsscript med tf-idf \n",
    "- lav recomendation funktion \n",
    "- wordclouds \n",
    "- lav notebooken lækker med beskrivelser af metoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data med mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Requirement already satisfied: spacy in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from -r requirements.txt (line 1)) (3.4.3)\n",
      "Requirement already satisfied: mrjob==0.7.4 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from -r requirements.txt (line 2)) (0.7.4)\n",
      "Requirement already satisfied: simplemma in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from -r requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from -r requirements.txt (line 4)) (4.64.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from -r requirements.txt (line 5)) (3.7)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from -r requirements.txt (line 6)) (1.8.2.2)\n",
      "Requirement already satisfied: sklearn in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from -r requirements.txt (line 7)) (0.0.post1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from mrjob==0.7.4->-r requirements.txt (line 2)) (6.0)\n",
      "Requirement already satisfied: click in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from nltk->-r requirements.txt (line 5)) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from nltk->-r requirements.txt (line 5)) (2022.10.31)\n",
      "Requirement already satisfied: joblib in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from nltk->-r requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (0.10.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (61.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (1.22.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (1.0.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (1.10.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (8.1.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (2.4.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (2.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (2.28.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from spacy->-r requirements.txt (line 1)) (3.0.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from tqdm->-r requirements.txt (line 4)) (0.4.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from packaging>=20.0->spacy->-r requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from pathy>=0.3.5->spacy->-r requirements.txt (line 1)) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy->-r requirements.txt (line 1)) (4.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 1)) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 1)) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 1)) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 1)) (2.0.12)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy->-r requirements.txt (line 1)) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy->-r requirements.txt (line 1)) (0.0.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from wordcloud->-r requirements.txt (line 6)) (3.5.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from wordcloud->-r requirements.txt (line 6)) (9.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from jinja2->spacy->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from matplotlib->wordcloud->-r requirements.txt (line 6)) (1.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from matplotlib->wordcloud->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from matplotlib->wordcloud->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from matplotlib->wordcloud->-r requirements.txt (line 6)) (4.33.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bayka\\anaconda3\\envs\\katrine_personal_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud->-r requirements.txt (line 6)) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.0.1; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\bayka\\Anaconda3\\envs\\katrine_personal_env\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>party</th>\n",
       "      <th>twitter_id</th>\n",
       "      <th>tweets</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>magnus_heunicke</td>\n",
       "      <td>socialdemokratiet</td>\n",
       "      <td>22695562</td>\n",
       "      <td>[afsætter året styrke hjælpen børn pårørende a...</td>\n",
       "      <td>[british, landsplan, baltisk, vist, missed, kv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nicolai_wammen</td>\n",
       "      <td>socialdemokratiet</td>\n",
       "      <td>2803948786</td>\n",
       "      <td>[dage siden sagde nyt ejendomsvurderingssystem...</td>\n",
       "      <td>[bent, mn, baltisk, vist, træ, splitte, elysee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mattias_tesfaye</td>\n",
       "      <td>socialdemokratiet</td>\n",
       "      <td>546254893</td>\n",
       "      <td>[this is literally the same logic many th c am...</td>\n",
       "      <td>[bent, snitte, vist, maradona, udlændingenævne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jakob_ellemann</td>\n",
       "      <td>venstre</td>\n",
       "      <td>155584627</td>\n",
       "      <td>[tide få fleksibel genåbning vores børn ældre ...</td>\n",
       "      <td>[stén, vrede, ministertid, vist, anne, varig, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>soren_gade</td>\n",
       "      <td>venstre</td>\n",
       "      <td>975064362359623680</td>\n",
       "      <td>[kære marianne synes burde læse lovforslaget i...</td>\n",
       "      <td>[tusindvis, mirakelkur, antisemitisme, betting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sophie_lohde</td>\n",
       "      <td>venstre</td>\n",
       "      <td>44611200</td>\n",
       "      <td>[flertallet veto dermed røde partier stort set...</td>\n",
       "      <td>[skræmmeka, des, vist, anmeldelse, underernæri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lars_lokke</td>\n",
       "      <td>moderaterne</td>\n",
       "      <td>26201346</td>\n",
       "      <td>[mon ikke sjov form argumentation mangler lidt...</td>\n",
       "      <td>[udgiftslov, bent, des, vist, folketingsspørgs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jacob_mark</td>\n",
       "      <td>sf</td>\n",
       "      <td>2373406198</td>\n",
       "      <td>[slår fast syvtommersøm kom så godt igennem fo...</td>\n",
       "      <td>[landsplan, snitte, vist, olieboring, dragsted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pia_dyhr</td>\n",
       "      <td>sf</td>\n",
       "      <td>65025162</td>\n",
       "      <td>[stemmer nok selvom synes gør godt klaus, brug...</td>\n",
       "      <td>[baltisk, vist, stefan, træ, hand, anne, samir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kirsten_andersen</td>\n",
       "      <td>sf</td>\n",
       "      <td>235646319</td>\n",
       "      <td>[arbejde få medarbejdere ser virkeligheden sun...</td>\n",
       "      <td>[bent, landsplan, julegudstjenester, vist, træ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dennis_flydtkjær</td>\n",
       "      <td>danmarksdemokraterne</td>\n",
       "      <td>531595033</td>\n",
       "      <td>[vel blot gældende forlig k i åbner det, europ...</td>\n",
       "      <td>[bent, gratisydelse, lovgrundlag, vist, dreami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>peter_skaarup</td>\n",
       "      <td>danmarksdemokraterne</td>\n",
       "      <td>3144074691</td>\n",
       "      <td>[justitsminister åbenbart svare på ogeller sik...</td>\n",
       "      <td>[bent, vrede, lovgrundlag, vist, anmeldelse, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>soren_espersen</td>\n",
       "      <td>danmarksdemokraterne</td>\n",
       "      <td>2444718215</td>\n",
       "      <td>[godt arbejde, det undre allermest forbindelse...</td>\n",
       "      <td>[bent, vrede, southern, british, baltisk, des,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>alex_vanopslagh</td>\n",
       "      <td>liberal_alliance</td>\n",
       "      <td>1531564633</td>\n",
       "      <td>[tror mest mennesket sætter gud prøve, mette f...</td>\n",
       "      <td>[des, landsplan, vist, dalgaard, anmeldelse, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ole_olesen</td>\n",
       "      <td>liberal_alliance</td>\n",
       "      <td>2222188479</td>\n",
       "      <td>[findes ingen talemåde slå stålet så andet ste...</td>\n",
       "      <td>[partigokkeriet, bent, podcastplatform, vist, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>soren_pape</td>\n",
       "      <td>konservative</td>\n",
       "      <td>2712091824</td>\n",
       "      <td>[tak mindst tak konstruktive input, dag god da...</td>\n",
       "      <td>[bent, vrede, ministertid, vist, anmeldelse, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>mette_abildgaard</td>\n",
       "      <td>konservative</td>\n",
       "      <td>37877392</td>\n",
       "      <td>[men kæmpe indsatsen lavere fjernvarmepriser n...</td>\n",
       "      <td>[bent, snitte, vist, klimaudsp, anne, træ, luk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>rasmus_jarlov</td>\n",
       "      <td>konservative</td>\n",
       "      <td>1225930531</td>\n",
       "      <td>[fuldstændig korrekt budskabet opslag sidste s...</td>\n",
       "      <td>[vist, læserbrev, varig, splitte, dråbe, kg, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pelle_dragsted</td>\n",
       "      <td>enhedslisten</td>\n",
       "      <td>119879630</td>\n",
       "      <td>[syriske mariam udvises gift jan venter barn l...</td>\n",
       "      <td>[vrede, affaldsforbrænding, emerita, monetaris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>mai_villadsen</td>\n",
       "      <td>enhedslisten</td>\n",
       "      <td>4724782641</td>\n",
       "      <td>[husk sexisme seksuel chikane magt arbejde gør...</td>\n",
       "      <td>[affaldsforbrænding, sommershoppingtur, landsp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rosa_lund</td>\n",
       "      <td>enhedslisten</td>\n",
       "      <td>736979161</td>\n",
       "      <td>[russiske flygtninge større chance få asyl ukr...</td>\n",
       "      <td>[bent, dirt, landsplan, anmeldelse, anne, indf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>martin_lidegaard</td>\n",
       "      <td>radikale</td>\n",
       "      <td>1070745218</td>\n",
       "      <td>[ja dokumentere fem år arbejdet indenfor felt ...</td>\n",
       "      <td>[bent, british, vrede, frema, olieudvinding, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>samira_nawa</td>\n",
       "      <td>radikale</td>\n",
       "      <td>92107029</td>\n",
       "      <td>[andet vigtigt klimarådet konkluderer dag lang...</td>\n",
       "      <td>[bent, dialogforums, snitte, vist, læserbrev, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>katrine_robsoe</td>\n",
       "      <td>radikale</td>\n",
       "      <td>2491403660</td>\n",
       "      <td>[tak samarbejdet, godt dkpol, vores uddannelse...</td>\n",
       "      <td>[vist, uddannelseskvaliteten, anne, varig, sam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>pernille_vermund</td>\n",
       "      <td>nye_borgerlige</td>\n",
       "      <td>24687777</td>\n",
       "      <td>[lars løkke varslede åbningstale flere udlændi...</td>\n",
       "      <td>[udgiftslov, intolerant, fnaftalen, vist, anme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>lars_mathiesen</td>\n",
       "      <td>nye_borgerlige</td>\n",
       "      <td>980721900</td>\n",
       "      <td>[siger del svagt enhedslisten reelt står rød b...</td>\n",
       "      <td>[bent, arguing, vrede, des, vist, varig, sugar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>kim_andersen</td>\n",
       "      <td>nye_borgerlige</td>\n",
       "      <td>783935815600799744</td>\n",
       "      <td>[vestlige erhvervsaktive alder stort underskud...</td>\n",
       "      <td>[baltisk, vist, træ, statministeren, ligning, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>franciska_rosenkilde</td>\n",
       "      <td>alternativet</td>\n",
       "      <td>777113466205274112</td>\n",
       "      <td>[sjøst sidste brug lige nu støjbergs sløje kul...</td>\n",
       "      <td>[splitte, flygtningelejr, biodiversitetskrisen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>christina_olumeko</td>\n",
       "      <td>alternativet</td>\n",
       "      <td>1324801335372488707</td>\n",
       "      <td>[rigtig ærgerligt socialdemokratiet dropper ar...</td>\n",
       "      <td>[vist, ut, bæredygtighed, valuta, parre, stærk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>torsten_gejl</td>\n",
       "      <td>alternativet</td>\n",
       "      <td>2806864609</td>\n",
       "      <td>[stolte kåringen, hej randahl, ja lille parti ...</td>\n",
       "      <td>[vist, træ, muliggøre, rovdrift, tange, flygtn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>morten_messerschmidt</td>\n",
       "      <td>dansk_folkeparti</td>\n",
       "      <td>509288627</td>\n",
       "      <td>[stort velkommen tilbage folketingsgruppen ige...</td>\n",
       "      <td>[baltisk, vist, stemnej, træ, splitte, treated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>pia_kjarsgaard</td>\n",
       "      <td>dansk_folkeparti</td>\n",
       "      <td>1054640354690039809</td>\n",
       "      <td>[nye borgerlige nedlægge hver femte offentlige...</td>\n",
       "      <td>[lovgrundlag, des, vist, stefan, stemnej, anne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>peter_kofod</td>\n",
       "      <td>dansk_folkeparti</td>\n",
       "      <td>1613378210</td>\n",
       "      <td>[kl slår gået galt stadig redde vort land kræv...</td>\n",
       "      <td>[vrede, vist, stemnej, splitte, plenarsalen, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                 party           twitter_id  \\\n",
       "0        magnus_heunicke     socialdemokratiet             22695562   \n",
       "1         nicolai_wammen     socialdemokratiet           2803948786   \n",
       "2        mattias_tesfaye     socialdemokratiet            546254893   \n",
       "3         jakob_ellemann               venstre            155584627   \n",
       "4             soren_gade               venstre   975064362359623680   \n",
       "5           sophie_lohde               venstre             44611200   \n",
       "6             lars_lokke           moderaterne             26201346   \n",
       "7             jacob_mark                    sf           2373406198   \n",
       "8               pia_dyhr                    sf             65025162   \n",
       "9       kirsten_andersen                    sf            235646319   \n",
       "10      dennis_flydtkjær  danmarksdemokraterne            531595033   \n",
       "11         peter_skaarup  danmarksdemokraterne           3144074691   \n",
       "12        soren_espersen  danmarksdemokraterne           2444718215   \n",
       "13       alex_vanopslagh      liberal_alliance           1531564633   \n",
       "14            ole_olesen      liberal_alliance           2222188479   \n",
       "15            soren_pape          konservative           2712091824   \n",
       "16      mette_abildgaard          konservative             37877392   \n",
       "17         rasmus_jarlov          konservative           1225930531   \n",
       "18        pelle_dragsted          enhedslisten            119879630   \n",
       "19         mai_villadsen          enhedslisten           4724782641   \n",
       "20             rosa_lund          enhedslisten            736979161   \n",
       "21      martin_lidegaard              radikale           1070745218   \n",
       "22           samira_nawa              radikale             92107029   \n",
       "23        katrine_robsoe              radikale           2491403660   \n",
       "24      pernille_vermund        nye_borgerlige             24687777   \n",
       "25        lars_mathiesen        nye_borgerlige            980721900   \n",
       "26          kim_andersen        nye_borgerlige   783935815600799744   \n",
       "27  franciska_rosenkilde          alternativet   777113466205274112   \n",
       "28     christina_olumeko          alternativet  1324801335372488707   \n",
       "29          torsten_gejl          alternativet           2806864609   \n",
       "30  morten_messerschmidt      dansk_folkeparti            509288627   \n",
       "31        pia_kjarsgaard      dansk_folkeparti  1054640354690039809   \n",
       "32           peter_kofod      dansk_folkeparti           1613378210   \n",
       "\n",
       "                                               tweets  \\\n",
       "0   [afsætter året styrke hjælpen børn pårørende a...   \n",
       "1   [dage siden sagde nyt ejendomsvurderingssystem...   \n",
       "2   [this is literally the same logic many th c am...   \n",
       "3   [tide få fleksibel genåbning vores børn ældre ...   \n",
       "4   [kære marianne synes burde læse lovforslaget i...   \n",
       "5   [flertallet veto dermed røde partier stort set...   \n",
       "6   [mon ikke sjov form argumentation mangler lidt...   \n",
       "7   [slår fast syvtommersøm kom så godt igennem fo...   \n",
       "8   [stemmer nok selvom synes gør godt klaus, brug...   \n",
       "9   [arbejde få medarbejdere ser virkeligheden sun...   \n",
       "10  [vel blot gældende forlig k i åbner det, europ...   \n",
       "11  [justitsminister åbenbart svare på ogeller sik...   \n",
       "12  [godt arbejde, det undre allermest forbindelse...   \n",
       "13  [tror mest mennesket sætter gud prøve, mette f...   \n",
       "14  [findes ingen talemåde slå stålet så andet ste...   \n",
       "15  [tak mindst tak konstruktive input, dag god da...   \n",
       "16  [men kæmpe indsatsen lavere fjernvarmepriser n...   \n",
       "17  [fuldstændig korrekt budskabet opslag sidste s...   \n",
       "18  [syriske mariam udvises gift jan venter barn l...   \n",
       "19  [husk sexisme seksuel chikane magt arbejde gør...   \n",
       "20  [russiske flygtninge større chance få asyl ukr...   \n",
       "21  [ja dokumentere fem år arbejdet indenfor felt ...   \n",
       "22  [andet vigtigt klimarådet konkluderer dag lang...   \n",
       "23  [tak samarbejdet, godt dkpol, vores uddannelse...   \n",
       "24  [lars løkke varslede åbningstale flere udlændi...   \n",
       "25  [siger del svagt enhedslisten reelt står rød b...   \n",
       "26  [vestlige erhvervsaktive alder stort underskud...   \n",
       "27  [sjøst sidste brug lige nu støjbergs sløje kul...   \n",
       "28  [rigtig ærgerligt socialdemokratiet dropper ar...   \n",
       "29  [stolte kåringen, hej randahl, ja lille parti ...   \n",
       "30  [stort velkommen tilbage folketingsgruppen ige...   \n",
       "31  [nye borgerlige nedlægge hver femte offentlige...   \n",
       "32  [kl slår gået galt stadig redde vort land kræv...   \n",
       "\n",
       "                                               tokens  \n",
       "0   [british, landsplan, baltisk, vist, missed, kv...  \n",
       "1   [bent, mn, baltisk, vist, træ, splitte, elysee...  \n",
       "2   [bent, snitte, vist, maradona, udlændingenævne...  \n",
       "3   [stén, vrede, ministertid, vist, anne, varig, ...  \n",
       "4   [tusindvis, mirakelkur, antisemitisme, betting...  \n",
       "5   [skræmmeka, des, vist, anmeldelse, underernæri...  \n",
       "6   [udgiftslov, bent, des, vist, folketingsspørgs...  \n",
       "7   [landsplan, snitte, vist, olieboring, dragsted...  \n",
       "8   [baltisk, vist, stefan, træ, hand, anne, samir...  \n",
       "9   [bent, landsplan, julegudstjenester, vist, træ...  \n",
       "10  [bent, gratisydelse, lovgrundlag, vist, dreami...  \n",
       "11  [bent, vrede, lovgrundlag, vist, anmeldelse, s...  \n",
       "12  [bent, vrede, southern, british, baltisk, des,...  \n",
       "13  [des, landsplan, vist, dalgaard, anmeldelse, s...  \n",
       "14  [partigokkeriet, bent, podcastplatform, vist, ...  \n",
       "15  [bent, vrede, ministertid, vist, anmeldelse, v...  \n",
       "16  [bent, snitte, vist, klimaudsp, anne, træ, luk...  \n",
       "17  [vist, læserbrev, varig, splitte, dråbe, kg, p...  \n",
       "18  [vrede, affaldsforbrænding, emerita, monetaris...  \n",
       "19  [affaldsforbrænding, sommershoppingtur, landsp...  \n",
       "20  [bent, dirt, landsplan, anmeldelse, anne, indf...  \n",
       "21  [bent, british, vrede, frema, olieudvinding, v...  \n",
       "22  [bent, dialogforums, snitte, vist, læserbrev, ...  \n",
       "23  [vist, uddannelseskvaliteten, anne, varig, sam...  \n",
       "24  [udgiftslov, intolerant, fnaftalen, vist, anme...  \n",
       "25  [bent, arguing, vrede, des, vist, varig, sugar...  \n",
       "26  [baltisk, vist, træ, statministeren, ligning, ...  \n",
       "27  [splitte, flygtningelejr, biodiversitetskrisen...  \n",
       "28  [vist, ut, bæredygtighed, valuta, parre, stærk...  \n",
       "29  [vist, træ, muliggøre, rovdrift, tange, flygtn...  \n",
       "30  [baltisk, vist, stemnej, træ, splitte, treated...  \n",
       "31  [lovgrundlag, des, vist, stefan, stemnej, anne...  \n",
       "32  [vrede, vist, stemnej, splitte, plenarsalen, i...  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "!python -m pip install -r requirements.txt\n",
    "from utils.initialization import *\n",
    "\n",
    "\n",
    "# med mrjob\n",
    "# names, party and twitter id\n",
    "from Data.twitter_ids import twitter_ids\n",
    "data = pd.DataFrame(columns=['name', \"party\", 'twitter_id'])\n",
    "i = 0\n",
    "for party in twitter_ids:\n",
    "    for person in twitter_ids[party]:\n",
    "        data.loc[i, :] = [person, party, twitter_ids[party][person]]\n",
    "        i += 1\n",
    "\n",
    "# tweets\n",
    "filename = \"Data/cleaned_data.csv\"\n",
    "if not os.path.exists(filename):\n",
    "    os.system(f\"python utils/clean_data_mrjob.py Data/tweets > Data/tmp_cleaned_data.txt\")\n",
    "    data_ = pd.DataFrame(columns=[\"name\", \"tweets\"])\n",
    "    with open(\"Data/tmp_cleaned_data.txt\", \"rb\") as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            line = eval(line.decode())\n",
    "            data_.loc[i,\"name\"] = list(line.keys())[0]\n",
    "            data_.loc[i, \"tweets\"] = list(line.values())[0]\n",
    "    data_.to_csv(filename, index = False)\n",
    "\n",
    "data_ = pd.read_csv(filename)\n",
    "data = data.merge(data_)\n",
    "data.tweets = [eval(t) for t in data.tweets]\n",
    "data[\"tokens\"] = [[w for w in word_tokenize(\" \".join(data[\"tweets\"][i])) if w.isalnum()] for i in range(len(data))]\n",
    "import simplemma #use simplemma instead of nltk.WordNetLemmatizer()          ### vi har addet denne linje\n",
    "data[\"tokens\"] = [[simplemma.lemmatize(w, lang='da') for w in data[\"tokens\"][i]] for i in range(len(data))] ### vi har addet denne linje\n",
    "\n",
    "# only include unique words\n",
    "data['tokens'] = data['tokens'].apply(lambda x: list(set(x)))\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_df = data #data.sample(5, random_state=42)\n",
    "# add party to the name\n",
    "mini_df['name'] = mini_df['name'] + ' ' + mini_df['party']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starter på at implementere solutions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingle(aString, q, delimiter=' '):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - aString (str): string to split into shingles\n",
    "        - q (int)\n",
    "        - delimiter (str): string of the delimiter to consider to split the input string (default: space)\n",
    "    Return: list of unique shingles\n",
    "    \"\"\"\n",
    "    all_shingles = []\n",
    "    if delimiter != '':\n",
    "        words_list = aString.split(delimiter)\n",
    "    else:\n",
    "        words_list = aString\n",
    "    for i in range (len(words_list)-q+1):\n",
    "        all_shingles.append(delimiter.join(words_list[i:i+q]))\n",
    "    return list(set(all_shingles))\n",
    "\n",
    "    \n",
    "# Example from the Book\n",
    "# ex_string, q = test, 3\n",
    "# ex_shingles = shingle(ex_string, q, delimiter='')\n",
    "# print('Initial string:', ex_string)\n",
    "# print(f'>> Shingles with q = {q} :',ex_shingles)\n",
    "\n",
    "# Example from the HINT\n",
    "for i in range(len(mini_df)):\n",
    "    ex_string, q = ' '.join(mini_df['tokens'][i]), 2\n",
    "    ex_shingles = shingle(ex_string, q)\n",
    "    # assert len(ex_shingles) == 7\n",
    "    # add shingle to the dataframe\n",
    "    mini_df.loc[i, 'shingles'] = str(ex_shingles)\n",
    "\n",
    "    # print('\\nInitial string:', ex_string)\n",
    "    # print(f'>> Shingles with q = {q} :',ex_shingles)\n",
    "\n",
    "# ex_string, q = 'test', 2\n",
    "# ex_shingles = shingle(ex_string, q)\n",
    "# assert len(ex_shingles) == 7\n",
    "# print('\\nInitial string:', ex_string)\n",
    "# print(f'>> Shingles with q = {q} :',ex_shingles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each shingle is a list of `q` words. We can use the `hash` function to convert each shingle into a number. We can then use the `min` function to find the smallest hash value for each document. This is the signature for the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lav TF-IDF til en kolonne i dataframen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the book (9.2.2) we can get the n most important (highest tfidf score) words PER document and treat them as a set\n",
    "# (unlike cosine similarity that treats the vectors with 0s and 1s for each time a word appears in both documents)\n",
    "#https://www.analyticsvidhya.com/blog/2021/12/how-to-extract-key-phrases-using-tfidf-with-python/\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=700, stop_words='english')\n",
    "vectors = vectorizer.fit_transform(mini_df.tokens.apply(lambda x: \" \".join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<33x700 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 21305 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_tokens={i[1]:i[0] for i in vectorizer.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents 33 the dictionary of document 1 677\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectors = []  # all deoc vectors by tfidf\n",
    "for row in vectors:\n",
    "  tfidf_vectors.append({dict_of_tokens[column]:value for (column,value) in zip(row.indices,row.data)})\n",
    "print(\"number of documents\",len(tfidf_vectors), \"the dictionary of document 1\", len(tfidf_vectors[3].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_df['TFIDF_Words'] = [list(tfidf_vectors[i].keys()) for i in range(len(mini_df))]\n",
    "# mini_df['TFIDF_Words'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minhashing algorithm\n",
    "Now we implement the minhashing algorithm. `minhash` that takes a list of shingles and a seed for the hash function\n",
    "mapping the shingles, and outputs the minhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import mmh3\n",
    "\n",
    "#hashes a list of strings\n",
    "def listhash(l,seed):\n",
    "\tval = 0\n",
    "\tfor e in l:\n",
    "\t\tval = val ^ mmh3.hash(e, seed)\n",
    "\treturn val \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO her tager vi istedet for shingles_list lister af tf-idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minhash(shingles_list, seed):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - shingles_list (list of str): set of hashes\n",
    "        - seed (int): seed for listhash function\n",
    "    Return: minhash of given shingles\n",
    "    \"\"\"\n",
    "    minhash_value = None\n",
    "    for aShingle in shingles_list:\n",
    "        hashcode = listhash([aShingle], seed)\n",
    "        if minhash_value == None or hashcode < minhash_value:\n",
    "            minhash_value = hashcode\n",
    "    return minhash_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'MinHash of mini_df.shingles[0]:', minhash(mini_df.shingles[4], 42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minhash2(shingles_list, k):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - shingles_list (list of str): set of hashes\n",
    "        - k (int): seed for listhash function\n",
    "    Return: sequence of k minhashes\n",
    "    \"\"\"\n",
    "    all_minhash = []\n",
    "    for i in range(k):\n",
    "        all_minhash.append(minhash(shingles_list, i))\n",
    "    return all_minhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k=42\n",
    "# print(f'MinHash of  with k = {k}:\\n', minhash2(mini_df.shingles[3], k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do this for all and save the minhash value to the dataframe \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(mini_df)):\n",
    "#     mini_df.loc[i, 'minhash_tokens'] = minhash(mini_df.tokens[i], 10)\n",
    "\n",
    "# for i in range(len(mini_df)):\n",
    "#     mini_df.loc[i, 'minhas_tf'] = minhash(mini_df.TFIDF_Words[i], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "# def signature(dict_docs, q = q, num_hashes = k):\n",
    "def signature(dict_docs, num_hashes = k):\n",
    "\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - dict_docs (dict of str:str): dictionary of {title:document}\n",
    "        - q (int)\n",
    "        - num_hashes (int)\n",
    "    Return: dictionary consisting of document id’s as keys and signatures as values\n",
    "    \"\"\"\n",
    "    dict_signatures = {}\n",
    "    total_texts = len(list(dict_docs.keys()))\n",
    "    counter = 1\n",
    "    for key,text in dict_docs.items():\n",
    "        print(f'{counter}/{total_texts} - {key} - Processing...')\n",
    "        # doc_shingles = shingle(text, q)\n",
    "        doc_shingles = mini_df.tokens[counter-1]\n",
    "        # doc_shingles = mini_df.TFIDF_Words[counter-1]\n",
    "        minhash_values = minhash2(doc_shingles, num_hashes)\n",
    "        dict_signatures[key] = minhash_values\n",
    "        counter += 1\n",
    "    return dict_signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/12 - socialdemokratiet - Processing...\n",
      "2/12 - venstre - Processing...\n",
      "3/12 - moderaterne - Processing...\n",
      "4/12 - sf - Processing...\n",
      "5/12 - danmarksdemokraterne - Processing...\n",
      "6/12 - liberal_alliance - Processing...\n",
      "7/12 - konservative - Processing...\n",
      "8/12 - enhedslisten - Processing...\n",
      "9/12 - radikale - Processing...\n",
      "10/12 - nye_borgerlige - Processing...\n",
      "11/12 - alternativet - Processing...\n",
      "12/12 - dansk_folkeparti - Processing...\n"
     ]
    }
   ],
   "source": [
    "# dict_docs is a dictionary of {name:TFIDF_Words}\n",
    "# dict_docs = {i:j for i,j in zip(mini_df['name'],mini_df['TFIDF_Words'])}\n",
    "# dict_docs = {i:j for i,j in zip(mini_df['name'],mini_df['tokens'])}\n",
    "dict_docs = {i:j for i,j in zip(mini_df['party'],mini_df['tokens'])}\n",
    "\n",
    "\n",
    "\n",
    "# signature_dict = signature(dict_docs, q = 2, num_hashes = 100)\n",
    "signature_dict = signature(dict_docs, num_hashes = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def jaccard_similarity(x,y):\n",
    "#   \"\"\" returns the jaccard similarity between two lists \"\"\"\n",
    "#   intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "#   union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "#   return intersection_cardinality/float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(name1, name2, signature_dict):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - name1 (str): key of the first document S\n",
    "        - name2 (str): key of the second document T\n",
    "        - signatures_dict (dict of str:list): dictionary of signatures\n",
    "    Return: Jaccard similarity between S and T\n",
    "    \"\"\"\n",
    "    signatures_doc1 = np.array(signature_dict[name1])\n",
    "    signatures_doc2 = np.array(signature_dict[name2])\n",
    "    # return np.sum(signatures_doc1 == signatures_doc2) #/ len(signatures_doc1)\n",
    "    return len(np.intersect1d(signatures_doc1, signatures_doc2))/len(np.union1d(signatures_doc1, signatures_doc2))#, np.setdiff1d(signatures_doc1, signatures_doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature_dict['soren_pape']\n",
    "# signature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard similarity between socialdemokratiet and socialdemokratiet: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "first_doc_key = mini_df.party[1]#list(docs.keys())[0]\n",
    "second_doc_key = mini_df.party[2]#list(docs.keys())[1]\n",
    "print(f'Jaccard similarity between {first_doc_key} and {second_doc_key}:', jaccard(first_doc_key, second_doc_key, signature_dict))#dict_signatures_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(signatures_dict, jaccard_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - signatures_dict (dict of str:list): dictionary of signatures\n",
    "        - jaccard_threshold (float): lower bound for Jaccard similarity score to consider\n",
    "            two documents as similar\n",
    "    Return: dictionary of similar items\n",
    "    \"\"\"\n",
    "    list_keys = list(signatures_dict.keys())\n",
    "    similar_items = {}\n",
    "    for i in range (len(list_keys)-1):\n",
    "        for j in range (i+1, len(list_keys)):\n",
    "            similarity_score = jaccard(list_keys[i], list_keys[j], signatures_dict)\n",
    "            if similarity_score >= jaccard_threshold:\n",
    "                similar_items[(list_keys[i], list_keys[j])] = similarity_score\n",
    "    return similar_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_similar_items = similar(signature_dict)\n",
    "# print('Found similar items:\\n', found_similar_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the most similar items\n",
    "most_similar_items = sorted(found_similar_items.items(), key=lambda x: x[1], reverse=True)\n",
    "# print('Most similar items:\\n', most_similar_items[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every person, return the 3 people most similar to him/her\n",
    "def most_similar_persons(similar_items, num_similar_persons=3):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - similar_items (dict of tuple:str): dictionary of similar items\n",
    "        - num_similar_persons (int): number of similar persons to return\n",
    "    Return: dictionary of most similar persons\n",
    "    \"\"\"\n",
    "    most_similar_persons = {}\n",
    "    for key,value in similar_items.items():\n",
    "        if key[0] not in most_similar_persons:\n",
    "            most_similar_persons[key[0]] = [(key[1], value)]\n",
    "        else:\n",
    "            most_similar_persons[key[0]].append((key[1], value))\n",
    "        if key[1] not in most_similar_persons:\n",
    "            most_similar_persons[key[1]] = [(key[0], value)]\n",
    "        else:\n",
    "            most_similar_persons[key[1]].append((key[0], value))\n",
    "    for key,value in most_similar_persons.items():\n",
    "        most_similar_persons[key] = sorted(value, key=lambda x: x[1], reverse=True)[:num_similar_persons]\n",
    "    return most_similar_persons\n",
    "\n",
    "most_similar_persons_res = most_similar_persons(found_similar_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Most similar person 1</th>\n",
       "      <th>Most similar person 2</th>\n",
       "      <th>Most similar person 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>socialdemokratiet</th>\n",
       "      <td>(nye_borgerlige, 0.1396011396011396)</td>\n",
       "      <td>(sf, 0.12359550561797752)</td>\n",
       "      <td>(enhedslisten, 0.12359550561797752)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>venstre</th>\n",
       "      <td>(alternativet, 0.14613180515759314)</td>\n",
       "      <td>(sf, 0.1267605633802817)</td>\n",
       "      <td>(radikale, 0.11731843575418995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moderaterne</th>\n",
       "      <td>(dansk_folkeparti, 0.13314447592067988)</td>\n",
       "      <td>(sf, 0.12994350282485875)</td>\n",
       "      <td>(liberal_alliance, 0.1267605633802817)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sf</th>\n",
       "      <td>(liberal_alliance, 0.19402985074626866)</td>\n",
       "      <td>(konservative, 0.17994100294985252)</td>\n",
       "      <td>(radikale, 0.17647058823529413)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liberal_alliance</th>\n",
       "      <td>(sf, 0.19402985074626866)</td>\n",
       "      <td>(radikale, 0.1695906432748538)</td>\n",
       "      <td>(nye_borgerlige, 0.16279069767441862)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>konservative</th>\n",
       "      <td>(sf, 0.17994100294985252)</td>\n",
       "      <td>(dansk_folkeparti, 0.15606936416184972)</td>\n",
       "      <td>(nye_borgerlige, 0.14942528735632185)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enhedslisten</th>\n",
       "      <td>(nye_borgerlige, 0.17647058823529413)</td>\n",
       "      <td>(sf, 0.15942028985507245)</td>\n",
       "      <td>(liberal_alliance, 0.14942528735632185)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radikale</th>\n",
       "      <td>(sf, 0.17647058823529413)</td>\n",
       "      <td>(nye_borgerlige, 0.17647058823529413)</td>\n",
       "      <td>(liberal_alliance, 0.1695906432748538)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nye_borgerlige</th>\n",
       "      <td>(enhedslisten, 0.17647058823529413)</td>\n",
       "      <td>(radikale, 0.17647058823529413)</td>\n",
       "      <td>(sf, 0.16279069767441862)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alternativet</th>\n",
       "      <td>(sf, 0.15273775216138327)</td>\n",
       "      <td>(venstre, 0.14613180515759314)</td>\n",
       "      <td>(liberal_alliance, 0.1396011396011396)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dansk_folkeparti</th>\n",
       "      <td>(sf, 0.15942028985507245)</td>\n",
       "      <td>(radikale, 0.15942028985507245)</td>\n",
       "      <td>(konservative, 0.15606936416184972)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>danmarksdemokraterne</th>\n",
       "      <td>(nye_borgerlige, 0.07816711590296496)</td>\n",
       "      <td>(enhedslisten, 0.07526881720430108)</td>\n",
       "      <td>(radikale, 0.06666666666666667)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Most similar person 1  \\\n",
       "socialdemokratiet        (nye_borgerlige, 0.1396011396011396)   \n",
       "venstre                   (alternativet, 0.14613180515759314)   \n",
       "moderaterne           (dansk_folkeparti, 0.13314447592067988)   \n",
       "sf                    (liberal_alliance, 0.19402985074626866)   \n",
       "liberal_alliance                    (sf, 0.19402985074626866)   \n",
       "konservative                        (sf, 0.17994100294985252)   \n",
       "enhedslisten            (nye_borgerlige, 0.17647058823529413)   \n",
       "radikale                            (sf, 0.17647058823529413)   \n",
       "nye_borgerlige            (enhedslisten, 0.17647058823529413)   \n",
       "alternativet                        (sf, 0.15273775216138327)   \n",
       "dansk_folkeparti                    (sf, 0.15942028985507245)   \n",
       "danmarksdemokraterne    (nye_borgerlige, 0.07816711590296496)   \n",
       "\n",
       "                                        Most similar person 2  \\\n",
       "socialdemokratiet                   (sf, 0.12359550561797752)   \n",
       "venstre                              (sf, 0.1267605633802817)   \n",
       "moderaterne                         (sf, 0.12994350282485875)   \n",
       "sf                        (konservative, 0.17994100294985252)   \n",
       "liberal_alliance               (radikale, 0.1695906432748538)   \n",
       "konservative          (dansk_folkeparti, 0.15606936416184972)   \n",
       "enhedslisten                        (sf, 0.15942028985507245)   \n",
       "radikale                (nye_borgerlige, 0.17647058823529413)   \n",
       "nye_borgerlige                (radikale, 0.17647058823529413)   \n",
       "alternativet                   (venstre, 0.14613180515759314)   \n",
       "dansk_folkeparti              (radikale, 0.15942028985507245)   \n",
       "danmarksdemokraterne      (enhedslisten, 0.07526881720430108)   \n",
       "\n",
       "                                        Most similar person 3  \n",
       "socialdemokratiet         (enhedslisten, 0.12359550561797752)  \n",
       "venstre                       (radikale, 0.11731843575418995)  \n",
       "moderaterne            (liberal_alliance, 0.1267605633802817)  \n",
       "sf                            (radikale, 0.17647058823529413)  \n",
       "liberal_alliance        (nye_borgerlige, 0.16279069767441862)  \n",
       "konservative            (nye_borgerlige, 0.14942528735632185)  \n",
       "enhedslisten          (liberal_alliance, 0.14942528735632185)  \n",
       "radikale               (liberal_alliance, 0.1695906432748538)  \n",
       "nye_borgerlige                      (sf, 0.16279069767441862)  \n",
       "alternativet           (liberal_alliance, 0.1396011396011396)  \n",
       "dansk_folkeparti          (konservative, 0.15606936416184972)  \n",
       "danmarksdemokraterne          (radikale, 0.06666666666666667)  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert most_similar_persons_res to a datamframe for easier visualization\n",
    "most_similar_persons_df = pd.DataFrame.from_dict(most_similar_persons_res, orient='index')\n",
    "most_similar_persons_df.columns = ['Most similar person 1', 'Most similar person 2', 'Most similar person 3']\n",
    "most_similar_persons_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make a group of names for each party (for visualization)\n",
    "def make_party_groups(df, party_column_name):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        - df (pd.DataFrame): dataframe with the party column\n",
    "        - party_column_name (str): name of the party column\n",
    "    Return: dictionary of party groups\n",
    "    \"\"\"\n",
    "    party_groups = {}\n",
    "    for party in df[party_column_name].unique():\n",
    "        party_groups[party] = list(df[df[party_column_name] == party].name)\n",
    "    return party_groups\n",
    "\n",
    "party_groups = make_party_groups(mini_df, 'party')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look at the party of the most similar items\n",
    "# similar_items_names = [i for i in found_similar_items.keys()]\n",
    "# for i in range(len(similar_items_names)):\n",
    "#     print(mini_df[mini_df.name.isin(similar_items_names[i])].party)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found similar items with LSH:\n",
      " {}\n"
     ]
    }
   ],
   "source": [
    "b,r = 20, 5\n",
    "# assert k == b*r\n",
    "\n",
    "def lsh(signatures_dict, jaccard_threshold=0.05, seed=10):\n",
    "    lsh_dict = {}\n",
    "    for key, values in signatures_dict.items():\n",
    "        blocks = np.split(np.array(values), b)\n",
    "        blocks_hash_values = []\n",
    "        for aBlock in blocks:\n",
    "            blocks_hash_values.append(mmh3.hash(aBlock, seed))\n",
    "        lsh_dict[key] = blocks_hash_values\n",
    "    list_keys = list(lsh_dict.keys())\n",
    "    similar_items = {}\n",
    "    for i in range (len(list_keys)-1):\n",
    "        for j in range (i+1, len(list_keys)):\n",
    "            common_values = np.intersect1d(lsh_dict[list_keys[i]], lsh_dict[list_keys[j]])\n",
    "            if len(common_values) > 0:\n",
    "                # we found a candidate\n",
    "                similarity_score = jaccard(list_keys[i], list_keys[j], signatures_dict)\n",
    "                if similarity_score >= jaccard_threshold:\n",
    "                    similar_items[(list_keys[i], list_keys[j])] = similarity_score\n",
    "    return similar_items\n",
    "found_similar_items_with_lsh = lsh(signature_dict)\n",
    "print('Found similar items with LSH:\\n', found_similar_items_with_lsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "cell_id": "00008-1cbe2894-0702-49cd-ac63-e2293e52639d",
    "deepnote_cell_height": 205,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# converting a string of text into a vector. Using teh transformer BERT model\n",
    "\n",
    "# Step one: Use BERT to convert our text into a vector\n",
    "# Step two:Get the cosine similarity (the cosine of the angle between the two vectors) \n",
    "    # of a fixed twitter profiles (vector) and all the other ones\n",
    "# Step three: Pick the twitter profiles (vectors) with the largest cosine similarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-22a0109a-dd72-4afc-ad56-803d99685b08",
    "deepnote_cell_height": 552,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/hands-on-content-based-recommender-system-using-python-1d643bf314e4\n",
    "\n",
    "def give_recommendations(index,print_recommendation = False,print_recommendation_plots= False,print_parties =False):\n",
    "  index_recomm =cos_sim_data.loc[index].sort_values(ascending=False).index.tolist()[1:10]\n",
    "  party_recomm =  giant_df['Party'].loc[index_recomm].values\n",
    "  result = {'PArty':party_recomm,'Index':index_recomm}\n",
    "  if print_recommendation==True:\n",
    "    print('The watched movie is this one: %s \\n'%(giant_df['Person'].loc[index]))\n",
    "    k=1\n",
    "    for movie in party_recomm:\n",
    "      print('The number %i recommended movie is this one: %s \\n'%(k,movie))\n",
    "  if print_recommendation_plots==True:\n",
    "    print('The plot of the watched movie is this one:\\n %s \\n'%(giant_df['CT'].loc[index]))\n",
    "    k=1\n",
    "    for q in range(len(party_recomm)):\n",
    "      plot_q = giant_df['Overview'].loc[index_recomm[q]]\n",
    "      print('The plot of the number %i recommended movie is this one:\\n %s \\n'%(k,plot_q))\n",
    "      k=k+1\n",
    "  if print_parties==True:\n",
    "    print('The party of the twitter profile is this one:\\n %s \\n'%(giant_df['Party'].loc[index]))\n",
    "    k=1\n",
    "    for q in range(len(party_recomm)):\n",
    "      plot_q = giant_df['Party'].loc[index_recomm[q]]\n",
    "      print('The plot of the number %i recommended party is this one:\\n %s \\n'%(k,plot_q))\n",
    "      k=k+1\n",
    "  return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00013-303c0e1b-a8d2-4a6d-8c4f-f394ba24a7e4",
    "deepnote_cell_height": 61,
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=30797f9c-952e-45b4-98d4-31c9ac73ae78' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "1584dd31-1e1c-4b66-9d76-016448129ed3",
  "kernelspec": {
   "display_name": "Python 3.8.13 ('katrine_personal_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23b47e8941e9532a227126a88d0aed60e854f3b3d5618484cace29efe7d4fdfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
